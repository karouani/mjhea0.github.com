
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Scraping Web Pages with Scrapy - Michael Herman</title>
  <meta name="author" content="Michael Herman">

  
  <meta name="description" content="This is a simple tutorial on how to write a crawler using Scrapy (BaseSpider) to scrape and parse Craigslist Nonprofit jobs in San Francisco and &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://mjhea0.github.com/blog/2012/11/05/scraping-web-pages-with-scrapy/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Michael Herman" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37074204-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Michael Herman</a></h1>
  
    <h2>get more from your data</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  

<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:mjhea0.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>

  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/youtube-tutorials">Youtube</a></li>
</ul>
</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Scraping Web Pages With Scrapy</h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-11-05T14:59:00-08:00" pubdate data-updated="true">Nov 5<span>th</span>, 2012</time>
        
      </p>
    
  </header>


<div class="entry-content"><p><em>This is a simple tutorial on how to write a crawler using Scrapy (BaseSpider) to scrape and parse Craigslist Nonprofit jobs in San Francisco and store the data to a CSV file. If you don&#8217;t have any experience with Scrapy, start by reading this <a href="http://doc.scrapy.org/en/latest/intro/tutorial.html">tutorial</a>. Also, I assume that you are familiar with Xpath; if not, please read the Xpath basic <a href="http://w3schools.com/xpath/">tutorial</a> on w3schools. Enjoy!</em></p>

<p><strong>Installation:</strong> Start by <a href="http://scrapy.org/">downloading</a> and installing Scrapy and all its dependencies. Refer to this <a href="http://www.youtube.com/watch?v=eEK2kmmvIdw">video</a>, if you need help.</p>

<p><strong>Create Project:</strong> Once installed, open your terminal and create a Scrapy project by navigating to the directory you&#8217;d like to store your project in and then running the following command:</p>

<pre><code>scrapy startproject craigslist_sample
</code></pre>

<p><strong>Item Class:</strong> Open the items.py within the ~craigslist_sample\craigslist_sample directory. Edit the items.py file to define the fields that you want contained with the Item. Since we want the post title and subsequent URL, the Item class looks like this:</p>

<pre><code># Define here the models for your scraped items

from scrapy.item import Item, Field

class CraigslistSampleItem(Item):
    title = Field()
    link = Field()
</code></pre>

<p><strong>The Spider:</strong> The spider defines the initial URL (http://sfbay.craigslist.org/npo/), how to follow links/pagination (if necessary), and how to extract and parse the fields defined above. The spider must define these attributes:</p>

<ul>
<li><em>name</em>: the spider&#8217;s unique identifier</li>
<li><em>start_urls</em>: URLs the spider begins crawling at</li>
<li><em>parse</em>: method that parses and extracts the scraped data, which will be called with the downloaded Response object of each start URL</li>
</ul>


<p>You also need to use the HtmlXpathSelector for working with Xpaths. Visit the Scrapy <a href="http://doc.scrapy.org/en/0.16/">tutorial</a> for more information. The following is the code for the basic spider:</p>

<pre><code>from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector
from craigslist_sample.items import CraigslistSampleItem

class MySpider(BaseSpider):
    name = "craig"
    allowed_domains = ["craigslist.org"]
    start_urls = ["http://sfbay.craigslist.org/npo/"]

    def parse(self, response):
        hxs = HtmlXPathSelector(response)
        titles = hxs.select("//p")
        for titles in titles:
            title = titles.select("a/text()").extract()
            link = titles.select("a/@href").extract()
            print title, link
</code></pre>

<p>Save this in the ~\craigslist_sample\craigslist_sample\spiders directory as test.py.</p>

<p><strong>Trial:</strong> Now you are ready for a trial run of the scraper. So, while in the root directory of your Scrapy project, run the following command to output the scraped data to the screen:</p>

<pre><code>scrapy crawl craig
</code></pre>

<p><strong>Dicts:</strong> The Item objects defined above are simply custom dicts. Use the standard dict syntax to return the extracted data inside the Item objects:</p>

<pre><code>item = CraigslistSampleItem()
item ["title"] = titles.select("a/text()").extract()
item ["link"] = titles.select("a/@href").extract()
</code></pre>

<p><strong>Release:</strong> Once complete, the final code looks like this:</p>

<pre><code>from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector
from craigslist_sample.items import CraigslistSampleItem

class MySpider(BaseSpider):
    name = "craig"
    allowed_domains = ["craigslist.org"]
    start_urls = ["http://sfbay.craigslist.org/npo/"]

    def parse(self, response):
        hxs = HtmlXPathSelector(response)
        titles = hxs.select("//p")
        items = []
        for titles in titles:
            item = CraigslistSampleItem()
            item ["title"] = titles.select("a/text()").extract()
            item ["link"] = titles.select("a/@href").extract()
            items.append(item)
        return items
</code></pre>

<p><strong>Store the data: </strong>The scraped data can now be <a href="http://doc.scrapy.org/en/0.16/topics/feed-exports.html#topics-feed-exports">stored</a> in these formats- JSON, CSV, and XML (among others). Run the following command to save the data in CSV:</p>

<pre><code>scrapy crawl craig -o items.csv -t csv
</code></pre>

<p>You should now have a CSV file in your directory called items.csv full of data:</p>

<p><img src="http://www.backwardsteps.com/uploads/2012-11-05_1411.png" alt="" /></p>

<p><em>Although this is relatively simple tutorial, there are still powerful things you can do by just customizing this basic script. Just remember to not overload the server on the website you are crawling. Scrapy allows you to set <a href="https://scrapy.readthedocs.org/en/latest/topics/settings.html?highlight=delay#download-delay">delays</a> to throttle the crawling speed.</em></p>

<hr />

<div class="embed-video-container"><iframe src="http://www.youtube.com/embed/1EFnX1UkXVU "></iframe></div>


<hr />

<p><em>In my next post I&#8217;ll show how to use Scrapy to  recursively crawl a site by following links. Until then, you can find the code for this project on <a href="https://github.com/mjhea0/Scrapy-Samples">Github</a>.</em></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Michael Herman</span></span>

      








  


<time datetime="2012-11-05T14:59:00-08:00" pubdate data-updated="true">Nov 5<span>th</span>, 2012</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/python/'>python</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://mjhea0.github.com/blog/2012/11/05/scraping-web-pages-with-scrapy/" data-via="" data-counturl="http://mjhea0.github.com/blog/2012/11/05/scraping-web-pages-with-scrapy/" >Tweet</a>
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2012/10/19/sentiment-analysis-feelings-not-facts/" title="Previous Post: Sentiment Analysis: Feelings, not Facts">&laquo; Sentiment Analysis: Feelings, not Facts</a>
      
      
        <a class="basic-alignment right" href="/blog/2012/11/08/recursively-scraping-web-pages-with-scrapy/" title="Next Post: Recursively Scraping Web Pages with Scrapy">Recursively Scraping Web Pages with Scrapy &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
	<br />
	<a class="sitelink" href="https://twitter.com/mikeherman" target="_blank"><img class="icon" src="https://raw.github.com/mjhea0/mjhea0.github.com/master/images/social/24px/twitter.png"/></a>
	<a href="http://www.linkedin.com/pub/michael-herman/3b/a94/4" target="_blank"><img alt="" src="https://raw.github.com/mjhea0/mjhea0.github.com/master/images/social/24px/linkedin.png"/></a>
	<a href="http://bit.ly/TzGt2K" target="_blank"><img alt="" src="https://raw.github.com/mjhea0/mjhea0.github.com/master/images/social/24px/youtube.png"/></a>
	<a href="https://github.com/mjhea0" target="_blank"><img alt="" src="https://raw.github.com/mjhea0/mjhea0.github.com/master/images/social/24px/github.png"/></a>
	
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/12/30/django-basics/">Django Basics</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/12/excel-tips-how-to-cut-down-on-calculations-using-sumif-and-sumifs/">Excel Tips: How to Cut Down on Calculations Using SUMIF and SUMIFS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/10/crash-course-in-web2py-part-5-modifying-the-appearance-and-deploying-the-web-form/">Crash Course in web2py (part 5 - modifying the appearance and deploying the web form)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/09/crash-course-in-web2py-part-4-managing-form-records/">Crash Course in web2py (part 4 - managing form records)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/06/crash-course-in-web2py-part-3-form-validation/">Crash Course in web2py (part 3 - form validation)</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Categories</h1>
  <ul id="categories">
    <li class='category'><a href='/blog/categories/analytics/'>analytics (5)</a></li>
<li class='category'><a href='/blog/categories/excel/'>excel (3)</a></li>
<li class='category'><a href='/blog/categories/python/'>python (9)</a></li>

  </ul>
</section><section>

</section>







  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Michael Herman &copy; 2012 -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span> -
  <a href="https://github.com/mjhea0/mjhea0.github.com">Fork me on Github</a>

</p>

</footer>
  



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
